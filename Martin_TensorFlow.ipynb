{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model, load_model, model_from_json\n",
    "from keras.layers import Dense, Embedding, Bidirectional, Input, TimeDistributed\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import sys\n",
    "import h5py\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, log_loss, f1_score\n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "from tensorflow.nn.rnn_cell import LSTMStateTuple\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.contrib.rnn import OutputProjectionWrapper\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import optim,nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "import pickle\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameters\n",
    "maxlen = 30\n",
    "labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "czech = pd.read_excel('czech.xlsx', encoding='latin',header = None)\n",
    "czech.columns = ['Name', 'm_or_f']\n",
    "czech['namelen'] = [len(str(i)) for i in czech['Name']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>m_or_f</th>\n",
       "      <th>namelen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abigail</td>\n",
       "      <td>f</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ada</td>\n",
       "      <td>f</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adalberta</td>\n",
       "      <td>f</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adéla</td>\n",
       "      <td>f</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelaida</td>\n",
       "      <td>f</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Name m_or_f  namelen\n",
       "0    Abigail      f        7\n",
       "1        Ada      f        3\n",
       "2  Adalberta      f        9\n",
       "3      Adéla      f        5\n",
       "4   Adelaida      f        8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cname = czech['Name']\n",
    "collect = []\n",
    "\n",
    "\n",
    "for i in range(len(cname)):\n",
    "    collect.extend(list(str(cname[i]).lower()))\n",
    "# collect.extend(['END'])\n",
    "collect = set(collect)\n",
    "\n",
    "czech.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.read_csv(\"gender_data.csv\",header=None)\n",
    "data_set.columns = ['name','m_or_f']\n",
    "data_set['namelen']= [len(str(i)) for i in data_set['name']]\n",
    "data_set1 = data_set[(data_set['namelen'] >= 2) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "m_or_f\n",
       "f    6705\n",
       "m    8475\n",
       "Name: name, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set1.groupby('m_or_f')['name'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = data_set['name']\n",
    "gender = data_set['m_or_f']\n",
    "vocab = set(' '.join([str(i) for i in names]))\n",
    "vocab.add('END')\n",
    "vocab = vocab.union(collect)\n",
    "len_vocab = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'h', 'a', 'i', 'č', 'ů', 'ó', '5', 'ž', 'ú', 'á', '7', 'š', 'y', 'ť', 'ě', 'ď', '0', 'u', '6', 'v', 'g', '.', 'END', 'ř', 't', 'z', '1', 'x', '3', '9', 'o', 'b', 'd', 'l', 'ý', 'm', 'f', 'n', 'e', 'p', '8', '4', 'q', '2', ' ', 'c', 'ň', 's', 'j', 'é', 'w', 'k', 'r', 'í'}\n",
      "vocab length is  54\n",
      "length of data_set is  15226\n"
     ]
    }
   ],
   "source": [
    "print(vocab)\n",
    "print(\"vocab length is \",len_vocab)\n",
    "print (\"length of data_set is \",len(data_set1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_index = dict((c, i) for i, c in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('char_index.txt', 'wb') as handle:\n",
    "#     pickle.dump(char_index, handle)\n",
    "\n",
    "with open('char_index.txt', 'rb') as handle:\n",
    "    char_index = pickle.loads(handle.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'h': 14, 'ň': 47, 'a': 45, 'i': 32, 'ů': 23, 'ó': 52, 'b': 25, '5': 40, 'ú': 26, 'á': 17, '7': 2, 'š': 36, 'y': 50, 'ť': 46, 'č': 38, 'ě': 24, 'ď': 13, '3': 49, '0': 44, 'u': 30, 'v': 19, '.': 34, '6': 12, 'END': 43, 'ř': 31, 't': 48, 'z': 3, '1': 41, 'x': 15, 'é': 20, '9': 7, 'o': 37, 'ý': 11, 'g': 9, 'l': 0, 'f': 27, 'n': 51, 'e': 1, 'p': 5, '8': 53, '4': 29, 'd': 16, 'q': 10, '2': 18, ' ': 22, 'c': 42, 'ž': 39, 's': 8, 'j': 21, 'w': 33, 'm': 28, 'k': 35, 'r': 6, 'í': 4}\n"
     ]
    }
   ],
   "source": [
    "print(char_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "np.random.seed(0)\n",
    "msk = np.random.rand(len(data_set1)) < 0.9\n",
    "train = data_set1[msk]\n",
    "test = data_set1[~msk]     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_flag(i):\n",
    "    tmp = np.zeros(len_vocab);\n",
    "    tmp[i] = 1\n",
    "    return(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_flag(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### modify the code above to also convert each index to one-hot encoded representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take data_set upto max and truncate rest\n",
    "#encode to vector space(one hot encoding)\n",
    "#padd 'END' to shorter sequences\n",
    "#also convert each index to one-hot encoding\n",
    "train_x = []\n",
    "train_y = []\n",
    "trunc_train_name = [str(i)[0:maxlen] for i in train.name]\n",
    "for i in trunc_train_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i)]\n",
    "    for k in range(0,maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index[\"END\"]))\n",
    "    train_x.append(tmp)\n",
    "for i in train.m_or_f:\n",
    "    if i == 'm':\n",
    "        train_y.append([1, 0])\n",
    "    else:\n",
    "        train_y.append([0, 1])\n",
    "    \n",
    "train_x=np.asarray(train_x)\n",
    "train_y=np.asarray(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = []\n",
    "test_y = []\n",
    "trunc_test_name = [str(i)[0:maxlen] for i in test.name]\n",
    "for i in trunc_test_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i)]\n",
    "    for k in range(0,maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index[\"END\"]))\n",
    "    test_x.append(tmp)\n",
    "for i in test.m_or_f:\n",
    "    if i == 'm':\n",
    "        test_y.append([1, 0])\n",
    "    else:\n",
    "        test_y.append([0, 1])\n",
    "    \n",
    "test_x = np.asarray(test_x)\n",
    "test_y = np.asarray(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13713, 30, 54)\n",
      "(1513, 2)\n"
     ]
    }
   ],
   "source": [
    "print(np.asarray(train_x).shape)\n",
    "print(np.asarray(test_y).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtrain_x = []\n",
    "vtrain_y = []\n",
    "\n",
    "train_name = [str(i) for i in czech.Name]\n",
    "for i in train_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i.lower())]\n",
    "    for k in range(0, maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index['END']))\n",
    "    vtrain_x.append(tmp)\n",
    "for i in czech.m_or_f:\n",
    "    if i == 'm':\n",
    "        vtrain_y.append([1,0])\n",
    "    else:\n",
    "        vtrain_y.append([0,1])\n",
    "vtrain_x = np.asarray(vtrain_x)\n",
    "vtrain_y = np.asarray(vtrain_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making BatchLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size of one instance from iter is:  torch.Size([600, 30, 54])\n",
      "Sample target size of that same one instance from iter is torch.Size([600, 2])\n"
     ]
    }
   ],
   "source": [
    "#Creating Tensor Datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "#Now creating Data Loaders\n",
    "batch = 600\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle = True, batch_size = batch)\n",
    "test_loader = DataLoader(test_data, shuffle = True, batch_size = batch)\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print(\"Sample input size of one instance from iter is: \", sample_x.size()) #Batch size, Sequence Length, Feature Dimension\n",
    "print(\"Sample target size of that same one instance from iter is\", sample_y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build model in Tensorflow ( a stacked LSTM model with many-to-one arch ) here 30 sequence and 2 output each for one category(m/f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Parameters '''\n",
    "tf.reset_default_graph()\n",
    "no_units=524 \n",
    "\n",
    "features=train_x.shape[-1] #30\n",
    "# new_phoneme=new_phoneme.reshape((-1,1,features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.placeholder(dtype = tf.float32, shape = [None, None , features], name = 'input')\n",
    "target = tf.placeholder(dtype = tf.int32, shape = [None, 2], name = 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_fw = [tf.nn.rnn_cell.LSTMCell(num_units=no_units, initializer=tf.keras.initializers.glorot_normal(), state_is_tuple=True)]\n",
    "encoder_bw = [tf.nn.rnn_cell.LSTMCell(num_units=no_units, initializer=tf.keras.initializers.glorot_normal(), state_is_tuple=True)]\n",
    "\n",
    "for i in range(1,3):\n",
    "\n",
    "    encoder_fw.append(tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.LSTMCell(num_units=no_units, initializer=tf.keras.initializers.glorot_normal(), state_is_tuple=True)]))\n",
    "    encoder_bw.append(tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.LSTMCell(num_units=no_units, initializer=tf.keras.initializers.glorot_normal(), state_is_tuple=True)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "encoder_outputs, encoder_fw_state, encoder_bw_state = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "                                                                           cells_fw = encoder_fw,\n",
    "                                                                           cells_bw = encoder_bw,\n",
    "                                                                           inputs = train,\n",
    "                                                                           dtype = tf.float32\n",
    "                                                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_outputs = tf.concat((encoder_fw_output, encoder_bw_output), 2)\n",
    "\n",
    "encoder_state_c = tf.concat((encoder_fw_state[-1][0].c, encoder_bw_state[-1][0].c), 1)\n",
    "\n",
    "encoder_state_h = tf.concat((encoder_fw_state[-1][0].h, encoder_bw_state[-1][0].h), 1)\n",
    "\n",
    "encoder_final_state = LSTMStateTuple(\n",
    "    c=encoder_state_c,\n",
    "    h=encoder_state_h\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=tf.layers.Dense(2)(encoder_state_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "losss= tf.losses.softmax_cross_entropy(target, output)\n",
    "trainop =  tf.train.AdamOptimizer(learning_rate=0.001).minimize(losss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch going is... 0  and current loss is... 0.6181861\n",
      "Current epoch going is... 1  and current loss is... 0.5131525\n",
      "Current epoch going is... 2  and current loss is... 0.45227897\n",
      "Current epoch going is... 3  and current loss is... 0.4561371\n",
      "Current epoch going is... 4  and current loss is... 0.4264607\n",
      "Current epoch going is... 5  and current loss is... 0.3748216\n",
      "Current epoch going is... 6  and current loss is... 0.38874\n",
      "Current epoch going is... 7  and current loss is... 0.36775187\n",
      "Current epoch going is... 8  and current loss is... 0.2857654\n",
      "Current epoch going is... 9  and current loss is... 0.2826659\n",
      "Current epoch going is... 10  and current loss is... 0.3081251\n",
      "Current epoch going is... 11  and current loss is... 0.30422148\n",
      "Current epoch going is... 12  and current loss is... 0.27230778\n",
      "Current epoch going is... 13  and current loss is... 0.27890605\n",
      "Current epoch going is... 14  and current loss is... 0.27448007\n",
      "Model saved in path: tmp/model_tensorflow.ckpt\n"
     ]
    }
   ],
   "source": [
    "losx=[]\n",
    "saver = tf.train.Saver()\n",
    "# with tf.device('/gp):\n",
    "# with tf.device('/gpu:0'):\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    if(tf.train.checkpoint_exists(\"tmp/model.ckpt\")):\n",
    "        saver.restore(sess, \"tmp/model.ckpt\")\n",
    "        print(\"Model Restored\")\n",
    "\n",
    "    for i in range(15):\n",
    "        for batch_j, target_j in train_loader:\n",
    "\n",
    "            loss,_=sess.run([losss,trainop],feed_dict={train: batch_j,\n",
    "                                                       target : target_j})\n",
    "            losx.append([loss])\n",
    "        print(\"Current epoch going is...\", i,\" and current loss is...\",loss)\n",
    "    save_path = saver.save(sess, \"tmp/model_tensorflow.ckpt\")\n",
    "    print(\"Model saved in path: %s\" % save_path)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ta = tf.placeholder(dtype= tf.float32, shape = [None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tmp/model_tensorflow.ckpt\n",
      "Model Restored\n",
      "real score 0.8766666666666667\n",
      "real score 0.8733333333333333\n",
      "real score 0.865814696485623\n",
      "Final mean is:  0.8719382321618744\n"
     ]
    }
   ],
   "source": [
    "accc = []\n",
    "# with tf.device('/gpu:0'):\n",
    "with tf.Session() as sess:\n",
    "\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "    if(tf.train.checkpoint_exists(\"tmp/model_tensorflow.ckpt\")):\n",
    "        saver.restore(sess, \"tmp/model_tensorflow.ckpt\")\n",
    "        print(\"Model Restored\")\n",
    "    for batch_j, target_j in test_loader:\n",
    "        out = sess.run([output], feed_dict={train: batch_j})\n",
    "        out = np.squeeze(out)\n",
    "#             out = np.round(out)\n",
    "#         print(\"This is\",sum([np.argmax(out[i])==np.argmax(target_j[i].numpy()) for i in range(target_j.shape[0])])/1000\n",
    "        metric = accuracy_score(np.argmax(target_j.numpy(), axis= 1),  np.argmax(np.squeeze(out), axis = 1))\n",
    "        print(\"real score\",metric )\n",
    "\n",
    "#         acc = tf.keras.backend.categorical_crossentropy(new_ta, out)\n",
    "        something = metric\n",
    "        something = np.squeeze(something)\n",
    "        accc.append(np.mean(something))\n",
    "    print(\"Final mean is: \",np.mean(accc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tmp/model_tensorflow.ckpt\n",
      "Martin ....it's male\n",
      "Martini ....It's female\n"
     ]
    }
   ],
   "source": [
    "name=[\"Martin\", \"Martini\"]\n",
    "X=[]\n",
    "trunc_name = [i[0:maxlen] for i in name]\n",
    "for i in trunc_name:\n",
    "    tmp = [set_flag(char_index[j.lower()]) for j in str(i)]\n",
    "    for k in range(0,maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index[\"END\"]))\n",
    "    X.append(tmp)\n",
    "with tf.Session() as sess:\n",
    "\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "    if(tf.train.checkpoint_exists(\"tmp/model_tensorflow.ckpt\")):\n",
    "        saver.restore(sess, \"tmp/model_tensorflow.ckpt\")\n",
    "    out = sess.run([output], feed_dict={train: np.asarray(X)})\n",
    "    out = np.squeeze(out)\n",
    "        \n",
    "out = np.argmax(out, axis=1)\n",
    "for i in range(out.shape[0]):\n",
    "    if out[i] == 0:\n",
    "        print(name[i], \"....it's male\")\n",
    "    else:\n",
    "        print(name[i],\"....It's female\")\n",
    "# pred=model.predict(np.asarray(X))\n",
    "# print(np.round(out[:,-1,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
