{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Embedding, Bidirectional, Input, TimeDistributed\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras import optimizers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import optim,nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameters\n",
    "maxlen = 30\n",
    "labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aaban</td>\n",
       "      <td>M</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aabha</td>\n",
       "      <td>F</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aabid</td>\n",
       "      <td>M</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aabriella</td>\n",
       "      <td>F</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aada</td>\n",
       "      <td>F</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name gender  probability\n",
       "0      Aaban      M          1.0\n",
       "1      Aabha      F          1.0\n",
       "2      Aabid      M          1.0\n",
       "3  Aabriella      F          1.0\n",
       "4       Aada      F          1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "czech = pd.read_excel('czech.xlsx', encoding='latin',header = None)\n",
    "czech.columns = ['Name', 'm_or_f']\n",
    "czech['namelen'] = [len(str(i)) for i in czech['Name']]\n",
    "\n",
    "\n",
    "ninety = pd.read_csv('name_gender.csv')\n",
    "ninety = ninety[(ninety['probability']>=0.75)]\n",
    "ninety.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing of new ninety file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ninety['m_or_f'] = list(map(lambda x:x.lower(), ninety['gender']))\n",
    "ninety['Name'] = list(map(lambda x:x.lower(), ninety['name']))\n",
    "\n",
    "ninety.drop(['name', 'probability', 'gender'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_name = ninety['Name']\n",
    "nollec = set(' '.join([str(i) for i in n_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>m_or_f</th>\n",
       "      <th>namelen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abigail</td>\n",
       "      <td>f</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ada</td>\n",
       "      <td>f</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adalberta</td>\n",
       "      <td>f</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adéla</td>\n",
       "      <td>f</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelaida</td>\n",
       "      <td>f</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Name m_or_f  namelen\n",
       "0    Abigail      f        7\n",
       "1        Ada      f        3\n",
       "2  Adalberta      f        9\n",
       "3      Adéla      f        5\n",
       "4   Adelaida      f        8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cname = czech['Name']\n",
    "collect = []\n",
    "\n",
    "\n",
    "for i in range(len(cname)):\n",
    "    collect.extend(list(str(cname[i]).lower()))\n",
    "# collect.extend(['END'])\n",
    "collect = set(collect)\n",
    "\n",
    "czech.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.read_csv(\"gender_data.csv\",header=None)\n",
    "data_set.columns = ['name','m_or_f']\n",
    "data_set['namelen']= [len(str(i)) for i in data_set['name']]\n",
    "data_set1 = data_set[(data_set['namelen'] >= 2) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "m_or_f\n",
       "f    59103\n",
       "m    33465\n",
       "Name: Name, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ninety.groupby('m_or_f')['Name'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = data_set['name']\n",
    "gender = data_set['m_or_f']\n",
    "vocab = set(' '.join([str(i) for i in names]))\n",
    "vocab.add('END')\n",
    "vocab = vocab.union(collect)\n",
    "vocab = vocab.union(nollec)\n",
    "len_vocab = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n', 'v', 'u', 'o', 'w', 'é', 'j', 'ž', 'l', 'ó', '0', '7', 'í', 'e', 'á', 'g', 'ú', 'a', 'ý', 'd', ' ', 'ť', 'c', '.', '3', 'x', 'ř', 'y', '1', 'ů', 'END', 't', 'i', 'h', '2', '9', 'ň', 'š', 'm', 'b', 'ď', 's', 'p', '4', '6', 'q', 'ě', 'r', 'f', 'z', 'č', '8', '5', 'k'}\n",
      "vocab length is  54\n",
      "length of data_set is  15226\n"
     ]
    }
   ],
   "source": [
    "print(vocab)\n",
    "print(\"vocab length is \",len_vocab)\n",
    "print (\"length of data_set is \",len(data_set1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_index = dict((c, i) for i, c in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'o': 3, 'u': 2, 'n': 0, 'é': 5, 'j': 6, 'v': 1, 'ž': 7, 'l': 8, 'h': 33, 'ó': 9, '0': 10, 'y': 27, '7': 11, 'í': 12, 'e': 13, 'á': 14, 'g': 15, 'ú': 16, '9': 35, 'd': 19, 'w': 4, '5': 52, ' ': 20, 'ť': 21, 'c': 22, '.': 23, 'f': 48, 'ř': 26, '3': 24, '1': 28, 'END': 30, 't': 31, 'i': 32, 'x': 25, '2': 34, 'a': 17, 'ň': 36, 'š': 37, 'm': 38, 'ď': 40, 's': 41, '4': 43, '6': 44, 'b': 39, 'ě': 46, 'r': 47, 'ý': 18, 'ů': 29, 'p': 42, 'z': 49, 'č': 50, '8': 51, 'q': 45, 'k': 53}\n"
     ]
    }
   ],
   "source": [
    "print(char_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "msk = np.random.rand(len(data_set1)) < 0.9\n",
    "train = data_set1[msk]\n",
    "test = data_set1[~msk]     \n",
    "\n",
    "msk = np.random.rand(len(czech)) < 0.9\n",
    "\n",
    "\n",
    "vtrain = czech[msk]\n",
    "vtest = czech[~msk]\n",
    "\n",
    "msk = np.random.rand(len(ninety)) < 0.9\n",
    "\n",
    "        \n",
    "ntrain = ninety[msk]\n",
    "ntest = ninety[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_flag(i):\n",
    "    tmp = np.zeros(len_vocab);\n",
    "    tmp[i] = 1\n",
    "    return(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_flag(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### modify the code above to also convert each index to one-hot encoded representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# These are set of indian names, just for adding a little robustness and to construct that latent structure in names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take data_set upto max and truncate rest\n",
    "#encode to vector space(one hot encoding)\n",
    "#padd 'END' to shorter sequences\n",
    "#also convert each index to one-hot encoding\n",
    "train_x = []\n",
    "train_y = []\n",
    "trunc_train_name = [str(i)[0:maxlen] for i in train.name]\n",
    "for i in trunc_train_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i)]\n",
    "    for k in range(0,maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index[\"END\"]))\n",
    "    train_x.append(tmp)\n",
    "for i in train.m_or_f:\n",
    "    if i == 'm':\n",
    "        train_y.append([1,0])\n",
    "    else:\n",
    "        train_y.append([0,1])\n",
    "    \n",
    "train_x=np.asarray(train_x)\n",
    "train_y=np.asarray(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = []\n",
    "test_y = []\n",
    "trunc_test_name = [str(i)[0:maxlen] for i in test.name]\n",
    "for i in trunc_test_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i)]\n",
    "    for k in range(0,maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index[\"END\"]))\n",
    "    test_x.append(tmp)\n",
    "for i in test.m_or_f:\n",
    "    if i == 'm':\n",
    "        test_y.append([1,0])\n",
    "    else:\n",
    "        test_y.append([0,1])\n",
    "    \n",
    "test_x = np.asarray(test_x)\n",
    "test_y = np.asarray(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# These are Czech names, well, that's what the program is all about "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtrain_x = []\n",
    "vtrain_y = []\n",
    "\n",
    "train_name = [str(i) for i in vtrain.Name]\n",
    "for i in train_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i.lower())]\n",
    "    for k in range(0, maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index['END']))\n",
    "    vtrain_x.append(tmp)\n",
    "for i in vtrain.m_or_f:\n",
    "    if i == 'm':\n",
    "        vtrain_y.append([1,0])\n",
    "    else:\n",
    "        vtrain_y.append([0,1])\n",
    "vtrain_x = np.asarray(vtrain_x)\n",
    "vtrain_y = np.asarray(vtrain_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtest_x = []\n",
    "vtest_y = []\n",
    "\n",
    "train_name = [str(i) for i in vtest.Name]\n",
    "for i in train_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i.lower())]\n",
    "    for k in range(0, maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index['END']))\n",
    "    vtest_x.append(tmp)\n",
    "for i in vtest.m_or_f:\n",
    "    if i == 'm':\n",
    "        vtest_y.append([1,0])\n",
    "    else:\n",
    "        vtest_y.append([0,1])\n",
    "vtest_x = np.asarray(vtest_x)\n",
    "vtest_y = np.asarray(vtest_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is just collection of names from all part of world, majority of my dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain_x = []\n",
    "ntrain_y = []\n",
    "\n",
    "train_name = [str(i) for i in ntrain.Name]\n",
    "for i in train_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i.lower())]\n",
    "    for k in range(0, maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index['END']))\n",
    "    ntrain_x.append(tmp)\n",
    "for i in ntrain.m_or_f:\n",
    "    if i == 'm':\n",
    "        ntrain_y.append([1,0])\n",
    "    else:\n",
    "        ntrain_y.append([0,1])\n",
    "ntrain_x = np.asarray(ntrain_x)\n",
    "ntrain_y = np.asarray(ntrain_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntest_x = []\n",
    "ntest_y = []\n",
    "\n",
    "train_name = [str(i) for i in ntest.Name]\n",
    "for i in train_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i.lower())]\n",
    "    for k in range(0, maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index['END']))\n",
    "    ntest_x.append(tmp)\n",
    "for i in ntest.m_or_f:\n",
    "    if i == 'm':\n",
    "        ntest_y.append([1,0])\n",
    "    else:\n",
    "        ntest_y.append([0,1])\n",
    "ntest_x = np.asarray(ntest_x)\n",
    "ntest_y = np.asarray(ntest_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining all training set, and all test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are all training data!\n",
      "Shape of indian training name: (13683, 30, 54)\n",
      "Shape of Czech training name: (1108, 30, 54)\n",
      "Shape of all world training name: (83218, 30, 54)\n"
     ]
    }
   ],
   "source": [
    "print(\"These are all training data!\")\n",
    "print(\"Shape of indian training name:\", train_x.shape)\n",
    "print(\"Shape of Czech training name:\", vtrain_x.shape)\n",
    "print('Shape of all world training name:',ntrain_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining them all...\n",
      "Now net shape is: (98009, 30, 54)\n",
      "Now they're combined, but we must shuffle them too...shuffling\n",
      "Shuffled!!\n"
     ]
    }
   ],
   "source": [
    "print(\"Combining them all...\")\n",
    "net_train_x = np.concatenate([train_x, vtrain_x, ntrain_x], axis = 0)\n",
    "net_train_y = np.concatenate([train_y, vtrain_y, ntrain_y], axis = 0)\n",
    "print(\"Now net shape is:\", net_train_x.shape)\n",
    "print(\"Now they're combined, but we must shuffle them too...shuffling\")\n",
    "np.random.shuffle(net_train_x)\n",
    "np.random.shuffle(net_train_y)\n",
    "print(\"Shuffled!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are all test data!\n",
      "Shape of indian test name: (1543, 30, 54)\n",
      "Shape of Czech test name: (142, 30, 54)\n",
      "Shape of all world test name: (9350, 30, 54)\n"
     ]
    }
   ],
   "source": [
    "print(\"These are all test data!\")\n",
    "print(\"Shape of indian test name:\", test_x.shape)\n",
    "print(\"Shape of Czech test name:\", vtest_x.shape)\n",
    "print('Shape of all world test name:',ntest_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining them all...\n",
      "Now net shape is: (11035, 30, 54)\n",
      "Now they're combined, but we must shuffle them too...shuffling\n",
      "Shuffled!!\n"
     ]
    }
   ],
   "source": [
    "print(\"Combining them all...\")\n",
    "net_test_x = np.concatenate([test_x, vtest_x, ntest_x], axis = 0)\n",
    "net_test_y = np.concatenate([test_y, vtest_y, ntest_y], axis = 0)\n",
    "print(\"Now net shape is:\", net_test_x.shape)\n",
    "print(\"Now they're combined, but we must shuffle them too...shuffling\")\n",
    "np.random.shuffle(net_test_x)\n",
    "np.random.shuffle(net_test_y)\n",
    "print(\"Shuffled!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model in PyTorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gender(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers):\n",
    "        super(gender, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first = True, bidirectional = True)\n",
    "        self.linear = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        lstmout, hidden = self.lstm.forward(x, hidden)\n",
    "        output = self.linear(lstmout)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.n_layers * 2, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers * 2, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mynet = gender(input_dim = 54, output_dim = 2, hidden_dim = 50, n_layers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainx, trainy, testx, testy, batch_size = 500, epochs = 50, lr = 0.01, clip = 5, print_every = 10):\n",
    "    \n",
    "    net.train()\n",
    "    net.double()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr = lr)\n",
    "    \n",
    "    net.cuda()\n",
    "    counter = 0\n",
    "    val_losses = []\n",
    "    train_permutation = torch.randperm(trainx.shape[0])\n",
    "    test_permutation = torch.randperm(testx.shape[0])\n",
    "\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        counter += 1\n",
    "        \n",
    "        #Initializing hidden state\n",
    "        hd = net.init_hidden(batch_size)\n",
    "        for i in range(0,trainx.shape[0], batch_size):\n",
    "            \n",
    "            indices = train_permutation[i:i+batch_size]\n",
    "\n",
    "        \n",
    "            inputs, targets = torch.from_numpy(trainx[indices]).cuda(), torch.from_numpy(trainy[indices]).cuda()\n",
    "\n",
    "            hd = tuple([each.data for each in hd])\n",
    "\n",
    "            net.zero_grad()\n",
    "\n",
    "            outputs, hd = net(inputs, hd)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if counter % print_every == 0:\n",
    "                net.eval()\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                \n",
    "                for j in range(0, testx.shape[0], batch_size):\n",
    "                    \n",
    "                    indic = test_permutation[j:j + batch_size]\n",
    "                    vinputs, vtargets = torch.from_numpy(testx[indic]).cuda(), torch.from_numpy(testy[indic]).cuda()\n",
    "\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                    output_h, val_h = net(vinputs, val_h)\n",
    "                    val_loss = criterion(output_h, vtargets)\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "\n",
    "            print(\"Epoch: {}/{} ...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                  \"Val Loss:{:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50 ... Step: 1... Loss: 3.4129... Val Loss:nan\n",
      "Epoch: 501/50 ... Step: 1... Loss: 3.3009... Val Loss:nan\n",
      "Epoch: 1001/50 ... Step: 1... Loss: 3.1246... Val Loss:nan\n",
      "Epoch: 1501/50 ... Step: 1... Loss: 2.7051... Val Loss:nan\n",
      "Epoch: 2001/50 ... Step: 1... Loss: 2.4606... Val Loss:nan\n",
      "Epoch: 2501/50 ... Step: 1... Loss: 3.0593... Val Loss:nan\n",
      "Epoch: 3001/50 ... Step: 1... Loss: 2.9980... Val Loss:nan\n",
      "Epoch: 3501/50 ... Step: 1... Loss: 3.0934... Val Loss:nan\n",
      "Epoch: 4001/50 ... Step: 1... Loss: 2.9725... Val Loss:nan\n",
      "Epoch: 4501/50 ... Step: 1... Loss: 2.5672... Val Loss:nan\n",
      "Epoch: 5001/50 ... Step: 1... Loss: 2.7931... Val Loss:nan\n",
      "Epoch: 5501/50 ... Step: 1... Loss: 2.4237... Val Loss:nan\n",
      "Epoch: 6001/50 ... Step: 1... Loss: 2.3198... Val Loss:nan\n",
      "Epoch: 6501/50 ... Step: 1... Loss: 2.2498... Val Loss:nan\n",
      "Epoch: 7001/50 ... Step: 1... Loss: 1.6523... Val Loss:nan\n",
      "Epoch: 7501/50 ... Step: 1... Loss: 1.6841... Val Loss:nan\n",
      "Epoch: 8001/50 ... Step: 1... Loss: 1.4204... Val Loss:nan\n",
      "Epoch: 8501/50 ... Step: 1... Loss: 1.4149... Val Loss:nan\n",
      "Epoch: 9001/50 ... Step: 1... Loss: 1.3890... Val Loss:nan\n",
      "Epoch: 9501/50 ... Step: 1... Loss: 1.4656... Val Loss:nan\n",
      "Epoch: 10001/50 ... Step: 1... Loss: 1.5758... Val Loss:nan\n",
      "Epoch: 10501/50 ... Step: 1... Loss: 1.5059... Val Loss:nan\n",
      "Epoch: 11001/50 ... Step: 1... Loss: 1.3684... Val Loss:nan\n",
      "Epoch: 11501/50 ... Step: 1... Loss: 1.3032... Val Loss:nan\n",
      "Epoch: 12001/50 ... Step: 1... Loss: 1.2468... Val Loss:nan\n",
      "Epoch: 12501/50 ... Step: 1... Loss: 1.2532... Val Loss:nan\n",
      "Epoch: 13001/50 ... Step: 1... Loss: 1.2256... Val Loss:nan\n",
      "Epoch: 13501/50 ... Step: 1... Loss: 1.1901... Val Loss:nan\n",
      "Epoch: 14001/50 ... Step: 1... Loss: 1.2178... Val Loss:nan\n",
      "Epoch: 14501/50 ... Step: 1... Loss: 1.1718... Val Loss:nan\n",
      "Epoch: 15001/50 ... Step: 1... Loss: 1.1644... Val Loss:nan\n",
      "Epoch: 15501/50 ... Step: 1... Loss: 1.1240... Val Loss:nan\n",
      "Epoch: 16001/50 ... Step: 1... Loss: 1.1448... Val Loss:nan\n",
      "Epoch: 16501/50 ... Step: 1... Loss: 1.0948... Val Loss:nan\n",
      "Epoch: 17001/50 ... Step: 1... Loss: 1.0357... Val Loss:nan\n",
      "Epoch: 17501/50 ... Step: 1... Loss: 1.0439... Val Loss:nan\n",
      "Epoch: 18001/50 ... Step: 1... Loss: 1.0596... Val Loss:nan\n",
      "Epoch: 18501/50 ... Step: 1... Loss: 1.0073... Val Loss:nan\n",
      "Epoch: 19001/50 ... Step: 1... Loss: 1.0153... Val Loss:nan\n",
      "Epoch: 19501/50 ... Step: 1... Loss: 0.9969... Val Loss:nan\n",
      "Epoch: 20001/50 ... Step: 1... Loss: 0.9658... Val Loss:nan\n",
      "Epoch: 20501/50 ... Step: 1... Loss: 0.9442... Val Loss:nan\n",
      "Epoch: 21001/50 ... Step: 1... Loss: 0.9180... Val Loss:nan\n",
      "Epoch: 21501/50 ... Step: 1... Loss: 0.9092... Val Loss:nan\n",
      "Epoch: 22001/50 ... Step: 1... Loss: 0.8906... Val Loss:nan\n",
      "Epoch: 22501/50 ... Step: 1... Loss: 0.8891... Val Loss:nan\n",
      "Epoch: 23001/50 ... Step: 1... Loss: 0.8961... Val Loss:nan\n",
      "Epoch: 23501/50 ... Step: 1... Loss: 0.8657... Val Loss:nan\n",
      "Epoch: 24001/50 ... Step: 1... Loss: 0.8450... Val Loss:nan\n",
      "Epoch: 24501/50 ... Step: 1... Loss: 0.8472... Val Loss:nan\n",
      "Epoch: 25001/50 ... Step: 1... Loss: 0.8698... Val Loss:nan\n",
      "Epoch: 25501/50 ... Step: 1... Loss: 0.8642... Val Loss:nan\n",
      "Epoch: 26001/50 ... Step: 1... Loss: 0.8395... Val Loss:nan\n",
      "Epoch: 26501/50 ... Step: 1... Loss: 0.8448... Val Loss:nan\n",
      "Epoch: 27001/50 ... Step: 1... Loss: 0.8407... Val Loss:nan\n",
      "Epoch: 27501/50 ... Step: 1... Loss: 0.8133... Val Loss:nan\n",
      "Epoch: 28001/50 ... Step: 1... Loss: 0.7850... Val Loss:nan\n",
      "Epoch: 28501/50 ... Step: 1... Loss: 0.7890... Val Loss:nan\n",
      "Epoch: 29001/50 ... Step: 1... Loss: 0.7720... Val Loss:nan\n",
      "Epoch: 29501/50 ... Step: 1... Loss: 0.7796... Val Loss:nan\n",
      "Epoch: 30001/50 ... Step: 1... Loss: 0.7432... Val Loss:nan\n",
      "Epoch: 30501/50 ... Step: 1... Loss: 0.7330... Val Loss:nan\n",
      "Epoch: 31001/50 ... Step: 1... Loss: 0.7305... Val Loss:nan\n",
      "Epoch: 31501/50 ... Step: 1... Loss: 0.7402... Val Loss:nan\n",
      "Epoch: 32001/50 ... Step: 1... Loss: 0.7124... Val Loss:nan\n",
      "Epoch: 32501/50 ... Step: 1... Loss: 0.7472... Val Loss:nan\n",
      "Epoch: 33001/50 ... Step: 1... Loss: 0.7428... Val Loss:nan\n",
      "Epoch: 33501/50 ... Step: 1... Loss: 0.7195... Val Loss:nan\n",
      "Epoch: 34001/50 ... Step: 1... Loss: 0.7280... Val Loss:nan\n",
      "Epoch: 34501/50 ... Step: 1... Loss: 0.7098... Val Loss:nan\n",
      "Epoch: 35001/50 ... Step: 1... Loss: 0.7365... Val Loss:nan\n",
      "Epoch: 35501/50 ... Step: 1... Loss: 0.7019... Val Loss:nan\n",
      "Epoch: 36001/50 ... Step: 1... Loss: 0.7237... Val Loss:nan\n",
      "Epoch: 36501/50 ... Step: 1... Loss: 0.6988... Val Loss:nan\n",
      "Epoch: 37001/50 ... Step: 1... Loss: 0.7237... Val Loss:nan\n",
      "Epoch: 37501/50 ... Step: 1... Loss: 0.6934... Val Loss:nan\n",
      "Epoch: 38001/50 ... Step: 1... Loss: 0.7110... Val Loss:nan\n",
      "Epoch: 38501/50 ... Step: 1... Loss: 0.6940... Val Loss:nan\n",
      "Epoch: 39001/50 ... Step: 1... Loss: 0.7463... Val Loss:nan\n",
      "Epoch: 39501/50 ... Step: 1... Loss: 0.7531... Val Loss:nan\n",
      "Epoch: 40001/50 ... Step: 1... Loss: 0.7184... Val Loss:nan\n",
      "Epoch: 40501/50 ... Step: 1... Loss: 0.6970... Val Loss:nan\n",
      "Epoch: 41001/50 ... Step: 1... Loss: 0.6847... Val Loss:nan\n",
      "Epoch: 41501/50 ... Step: 1... Loss: 0.7084... Val Loss:nan\n",
      "Epoch: 42001/50 ... Step: 1... Loss: 0.7200... Val Loss:nan\n",
      "Epoch: 42501/50 ... Step: 1... Loss: 0.6989... Val Loss:nan\n",
      "Epoch: 43001/50 ... Step: 1... Loss: 0.6998... Val Loss:nan\n",
      "Epoch: 43501/50 ... Step: 1... Loss: 0.6797... Val Loss:nan\n",
      "Epoch: 44001/50 ... Step: 1... Loss: 0.7049... Val Loss:nan\n",
      "Epoch: 44501/50 ... Step: 1... Loss: 0.6916... Val Loss:nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-9b0f5da39388>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmynet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_train_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_train_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_test_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_test_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-a3af44de24c0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, trainx, trainy, testx, testy, batch_size, epochs, lr, clip, print_every)\u001b[0m\n\u001b[1;32m     56\u001b[0m             print(\"Epoch: {}/{} ...\".format(i+1, epochs),\n\u001b[1;32m     57\u001b[0m                   \u001b[0;34m\"Step: {}...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                   \u001b[0;34m\"Loss: {:.4f}...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                   \"Val Loss:{:.4f}\".format(np.mean(val_losses)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden = train(mynet, trainx = net_train_x, trainy = net_train_y, testx = net_test_x, testy = net_test_y, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build model in keras ( a stacked LSTM model with many-to-one arch ) here 30 sequence and 2 output each for one category(m/f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #build the model: 2 stacked LSTM\n",
    "# print('Build model...')\n",
    "# input_bilstm=Input(shape = (maxlen,len_vocab))\n",
    "# bi_one = Bidirectional(LSTM(1024, return_sequences=True))(input_bilstm)\n",
    "# drop1 = Dropout(0.2)(bi_one)\n",
    "# bi_two = Bidirectional(LSTM(1024, return_sequences=False))(drop1)\n",
    "# drop2 = Dropout(0.2)(bi_two)\n",
    "# output = Dense(2, activation='softmax')(drop2)\n",
    "# model = Model(input_bilstm, output)\n",
    "\n",
    "\n",
    "# opt = optimizers.adam(lr = 0.01)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#build the model: 2 stacked LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(maxlen,len_vocab)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "opt = optimizers.SGD(lr=0.1)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=500\n",
    "model.fit(net_train_x, net_train_y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=50,\n",
    "          validation_data=(net_test_x, net_test_y)\n",
    "         )\n",
    "model.save('Martin_program.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc = model.evaluate(vtest_x, vtest_y)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30979005, 0.6902099 ]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name=[\"riya\"]\n",
    "X=[]\n",
    "trunc_name = [i[0:maxlen] for i in name]\n",
    "for i in trunc_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i.lower())]\n",
    "    for k in range(0,maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index[\"END\"]))\n",
    "    X.append(tmp)\n",
    "pred=model.predict(np.asarray(X))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30979005, 0.6902099 ]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets train more, clearly some very simple female names it doesnt get right like mentioned above (inspite it exists in training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-22f80a43d14b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test score:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_X' is not defined"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(test_X, test_Y)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30979005, 0.69021   ],\n",
       "       [0.30979005, 0.69021   ],\n",
       "       [0.30979005, 0.69021   ],\n",
       "       [0.30979005, 0.69021   ],\n",
       "       [0.30979005, 0.69021   ],\n",
       "       [0.30979005, 0.69021   ],\n",
       "       [0.30979005, 0.69021   ],\n",
       "       [0.30979005, 0.69021   ],\n",
       "       [0.30979005, 0.69021   ]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name=[\"sandhya\",\"jaspreet\",\"rajesh\",\"kaveri\",\"aditi deepak\",\"arihant\",\"sasikala\",\"aditi\",\"ragini rajaram\"]\n",
    "X=[]\n",
    "trunc_name = [i[0:maxlen] for i in name]\n",
    "for i in trunc_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i)]\n",
    "    for k in range(0,maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index[\"END\"]))\n",
    "    X.append(tmp)\n",
    "pred=model.predict(np.asarray(X))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4178515 , 0.5821485 ],\n",
       "       [0.37405205, 0.62594795],\n",
       "       [0.02188767, 0.9781123 ]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name=[\"abhi\",\"abhi deepak\",\"mr. abhi\"]\n",
    "X=[]\n",
    "trunc_name = [i[0:maxlen] for i in name]\n",
    "for i in trunc_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i)]\n",
    "    for k in range(0,maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index[\"END\"]))\n",
    "    X.append(tmp)\n",
    "pred=model.predict(np.asarray(X))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09026915, 0.90973085],\n",
       "       [0.9785337 , 0.02146639],\n",
       "       [0.03325909, 0.9667409 ]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name=[\"rajini\",\"rajinikanth\",\"mr. rajini\"]\n",
    "X=[]\n",
    "trunc_name = [i[0:maxlen] for i in name]\n",
    "for i in trunc_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i)]\n",
    "    for k in range(0,maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index[\"END\"]))\n",
    "    X.append(tmp)\n",
    "pred=model.predict(np.asarray(X))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save our model and data\n",
    "model.save_weights('gender_model',overwrite=True)\n",
    "train.to_csv(\"train_split.csv\")\n",
    "test.to_csv(\"test_split.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = model.predict(test_X)\n",
    "prob_m = [i[0] for i in evals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.DataFrame(prob_m)\n",
    "out['name'] = test.name.reset_index()['name']\n",
    "out['m_or_f']=test.m_or_f.reset_index()['m_or_f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.head(10)\n",
    "out.columns = ['prob_m','name','actual']\n",
    "out.head(10)\n",
    "out.to_csv(\"gender_pred_out.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
