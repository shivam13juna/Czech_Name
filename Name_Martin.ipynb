{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Embedding, Bidirectional, Input, TimeDistributed\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras import optimizers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import optim,nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameters\n",
    "maxlen = 30\n",
    "labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aaban</td>\n",
       "      <td>M</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aabha</td>\n",
       "      <td>F</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aabid</td>\n",
       "      <td>M</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aabriella</td>\n",
       "      <td>F</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aada</td>\n",
       "      <td>F</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name gender  probability\n",
       "0      Aaban      M          1.0\n",
       "1      Aabha      F          1.0\n",
       "2      Aabid      M          1.0\n",
       "3  Aabriella      F          1.0\n",
       "4       Aada      F          1.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "czech = pd.read_excel('czech.xlsx', encoding='latin',header = None)\n",
    "czech.columns = ['Name', 'm_or_f']\n",
    "czech['namelen'] = [len(str(i)) for i in czech['Name']]\n",
    "\n",
    "\n",
    "ninety = pd.read_csv('name_gender.csv')\n",
    "ninety = ninety[(ninety['probability']>=0.75)]\n",
    "ninety.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing of new ninety file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ninety['m_or_f'] = list(map(lambda x:x.lower(), ninety['gender']))\n",
    "ninety['Name'] = list(map(lambda x:x.lower(), ninety['name']))\n",
    "\n",
    "ninety.drop(['name', 'probability', 'gender'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_name = ninety['Name']\n",
    "nollec = set(' '.join([str(i) for i in n_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>m_or_f</th>\n",
       "      <th>namelen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abigail</td>\n",
       "      <td>f</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ada</td>\n",
       "      <td>f</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adalberta</td>\n",
       "      <td>f</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adéla</td>\n",
       "      <td>f</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelaida</td>\n",
       "      <td>f</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Name m_or_f  namelen\n",
       "0    Abigail      f        7\n",
       "1        Ada      f        3\n",
       "2  Adalberta      f        9\n",
       "3      Adéla      f        5\n",
       "4   Adelaida      f        8"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cname = czech['Name']\n",
    "collect = []\n",
    "\n",
    "\n",
    "for i in range(len(cname)):\n",
    "    collect.extend(list(str(cname[i]).lower()))\n",
    "# collect.extend(['END'])\n",
    "collect = set(collect)\n",
    "\n",
    "czech.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.read_csv(\"gender_data.csv\",header=None)\n",
    "data_set.columns = ['name','m_or_f']\n",
    "data_set['namelen']= [len(str(i)) for i in data_set['name']]\n",
    "data_set1 = data_set[(data_set['namelen'] >= 2) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "m_or_f\n",
       "f    59103\n",
       "m    33465\n",
       "Name: Name, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ninety.groupby('m_or_f')['Name'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = data_set['name']\n",
    "gender = data_set['m_or_f']\n",
    "vocab = set(' '.join([str(i) for i in names]))\n",
    "vocab.add('END')\n",
    "vocab = vocab.union(collect)\n",
    "vocab = vocab.union(nollec)\n",
    "len_vocab = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'č', 'o', 'u', 'á', 'ě', 'r', 'f', 'ň', 'b', 'h', 'ř', 'é', 'j', '.', '4', 'i', 'p', 'a', 'q', 'ž', '2', 'c', '9', 'l', 'v', 'g', '3', 'ť', 'k', 'w', 'x', 'y', '8', 'ý', ' ', 's', 'ů', 'm', 'í', 'z', 'e', '7', '0', 't', '5', 'd', 'n', 'ó', 'ď', 'š', '6', 'ú', '1', 'END'}\n",
      "vocab length is  54\n",
      "length of data_set is  15226\n"
     ]
    }
   ],
   "source": [
    "print(vocab)\n",
    "print(\"vocab length is \",len_vocab)\n",
    "print (\"length of data_set is \",len(data_set1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_index = dict((c, i) for i, c in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'č': 0, 'o': 1, 'u': 2, 'á': 3, 'ě': 4, 'r': 5, 'f': 6, 'ň': 7, 'b': 8, 'h': 9, 'ř': 10, 'é': 11, 'j': 12, '.': 13, '4': 14, 'i': 15, 'p': 16, 'a': 17, 'q': 18, '2': 20, 'c': 21, '9': 22, 'l': 23, '0': 42, 'v': 24, 'g': 25, '3': 26, 'ť': 27, 'k': 28, 'w': 29, 'e': 40, 'y': 31, 'ý': 33, ' ': 34, 's': 35, 'ů': 36, 'm': 37, 'í': 38, 'z': 39, '7': 41, 'END': 53, 't': 43, 'ž': 19, 'x': 30, 'd': 45, 'n': 46, 'ó': 47, 'š': 49, '6': 50, 'ú': 51, '1': 52, 'ď': 48, '5': 44, '8': 32}\n"
     ]
    }
   ],
   "source": [
    "print(char_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "msk = np.random.rand(len(data_set1)) < 0.9\n",
    "train = data_set1[msk]\n",
    "test = data_set1[~msk]     \n",
    "\n",
    "msk = np.random.rand(len(czech)) < 0.9\n",
    "\n",
    "\n",
    "vtrain = czech[msk]\n",
    "vtest = czech[~msk]\n",
    "\n",
    "msk = np.random.rand(len(ninety)) < 0.9\n",
    "\n",
    "        \n",
    "ntrain = ninety[msk]\n",
    "ntest = ninety[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_flag(i):\n",
    "    tmp = np.zeros(len_vocab);\n",
    "    tmp[i] = 1\n",
    "    return(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_flag(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### modify the code above to also convert each index to one-hot encoded representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# These are set of indian names, just for adding a little robustness and to construct that latent structure in names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take data_set upto max and truncate rest\n",
    "#encode to vector space(one hot encoding)\n",
    "#padd 'END' to shorter sequences\n",
    "#also convert each index to one-hot encoding\n",
    "train_x = []\n",
    "train_y = []\n",
    "trunc_train_name = [str(i)[0:maxlen] for i in train.name]\n",
    "for i in trunc_train_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i)]\n",
    "    for k in range(0,maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index[\"END\"]))\n",
    "    train_x.append(tmp)\n",
    "for i in train.m_or_f:\n",
    "    if i == 'm':\n",
    "        train_y.append([1,0])\n",
    "    else:\n",
    "        train_y.append([0,1])\n",
    "    \n",
    "train_x=np.asarray(train_x)\n",
    "train_y=np.asarray(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = []\n",
    "test_y = []\n",
    "trunc_test_name = [str(i)[0:maxlen] for i in test.name]\n",
    "for i in trunc_test_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i)]\n",
    "    for k in range(0,maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index[\"END\"]))\n",
    "    test_x.append(tmp)\n",
    "for i in test.m_or_f:\n",
    "    if i == 'm':\n",
    "        test_y.append([1,0])\n",
    "    else:\n",
    "        test_y.append([0,1])\n",
    "    \n",
    "test_x = np.asarray(test_x)\n",
    "test_y = np.asarray(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# These are Czech names, well, that's what the program is all about "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtrain_x = []\n",
    "vtrain_y = []\n",
    "\n",
    "train_name = [str(i) for i in vtrain.Name]\n",
    "for i in train_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i.lower())]\n",
    "    for k in range(0, maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index['END']))\n",
    "    vtrain_x.append(tmp)\n",
    "for i in vtrain.m_or_f:\n",
    "    if i == 'm':\n",
    "        vtrain_y.append([1,0])\n",
    "    else:\n",
    "        vtrain_y.append([0,1])\n",
    "vtrain_x = np.asarray(vtrain_x)\n",
    "vtrain_y = np.asarray(vtrain_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtest_x = []\n",
    "vtest_y = []\n",
    "\n",
    "train_name = [str(i) for i in vtest.Name]\n",
    "for i in train_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i.lower())]\n",
    "    for k in range(0, maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index['END']))\n",
    "    vtest_x.append(tmp)\n",
    "for i in vtest.m_or_f:\n",
    "    if i == 'm':\n",
    "        vtest_y.append([1,0])\n",
    "    else:\n",
    "        vtest_y.append([0,1])\n",
    "vtest_x = np.asarray(vtest_x)\n",
    "vtest_y = np.asarray(vtest_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is just collection of names from all part of world, majority of my dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain_x = []\n",
    "ntrain_y = []\n",
    "\n",
    "train_name = [str(i) for i in ntrain.Name]\n",
    "for i in train_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i.lower())]\n",
    "    for k in range(0, maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index['END']))\n",
    "    ntrain_x.append(tmp)\n",
    "for i in ntrain.m_or_f:\n",
    "    if i == 'm':\n",
    "        ntrain_y.append([1,0])\n",
    "    else:\n",
    "        ntrain_y.append([0,1])\n",
    "ntrain_x = np.asarray(ntrain_x)\n",
    "ntrain_y = np.asarray(ntrain_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntest_x = []\n",
    "ntest_y = []\n",
    "\n",
    "train_name = [str(i) for i in ntest.Name]\n",
    "for i in train_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i.lower())]\n",
    "    for k in range(0, maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index['END']))\n",
    "    ntest_x.append(tmp)\n",
    "for i in ntest.m_or_f:\n",
    "    if i == 'm':\n",
    "        ntest_y.append([1,0])\n",
    "    else:\n",
    "        ntest_y.append([0,1])\n",
    "ntest_x = np.asarray(ntest_x)\n",
    "ntest_y = np.asarray(ntest_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining all training set, and all test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are all training data!\n",
      "Shape of indian training name: (13776, 30, 54)\n",
      "Shape of Czech training name: (1132, 30, 54)\n",
      "Shape of all world training name: (83196, 30, 54)\n"
     ]
    }
   ],
   "source": [
    "print(\"These are all training data!\")\n",
    "print(\"Shape of indian training name:\", train_x.shape)\n",
    "print(\"Shape of Czech training name:\", vtrain_x.shape)\n",
    "print('Shape of all world training name:',ntrain_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining them all...\n",
      "Now net shape is: (98104, 30, 54)\n",
      "Now they're combined, but we must shuffle them too...shuffling\n",
      "Shuffled!!\n"
     ]
    }
   ],
   "source": [
    "print(\"Combining them all...\")\n",
    "net_train_x = np.concatenate([train_x, vtrain_x, ntrain_x], axis = 0)\n",
    "net_train_y = np.concatenate([train_y, vtrain_y, ntrain_y], axis = 0)\n",
    "print(\"Now net shape is:\", net_train_x.shape)\n",
    "print(\"Now they're combined, but we must shuffle them too...shuffling\")\n",
    "np.random.shuffle(net_train_x)\n",
    "np.random.shuffle(net_train_y)\n",
    "print(\"Shuffled!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are all test data!\n",
      "Shape of indian test name: (1450, 30, 54)\n",
      "Shape of Czech test name: (118, 30, 54)\n",
      "Shape of all world test name: (9372, 30, 54)\n"
     ]
    }
   ],
   "source": [
    "print(\"These are all test data!\")\n",
    "print(\"Shape of indian test name:\", test_x.shape)\n",
    "print(\"Shape of Czech test name:\", vtest_x.shape)\n",
    "print('Shape of all world test name:',ntest_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining them all...\n",
      "Now net shape is: (10940, 30, 54)\n",
      "Now they're combined, but we must shuffle them too...shuffling\n",
      "Shuffled!!\n"
     ]
    }
   ],
   "source": [
    "print(\"Combining them all...\")\n",
    "net_test_x = np.concatenate([test_x, vtest_x, ntest_x], axis = 0)\n",
    "net_test_y = np.concatenate([test_y, vtest_y, ntest_y], axis = 0)\n",
    "print(\"Now net shape is:\", net_test_x.shape)\n",
    "print(\"Now they're combined, but we must shuffle them too...shuffling\")\n",
    "np.random.shuffle(net_test_x)\n",
    "np.random.shuffle(net_test_y)\n",
    "print(\"Shuffled!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model in PyTorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gender(nn.Module):\n",
    "    def __init__(self, input_dim = 54, hidden_dim = 524, output_dim = 2, n_layers = 2, lr = 0.001):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first = True, bidirectional = True)\n",
    "        self.linear = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        lstmout, hidden = self.lstm.forward(x, hidden)\n",
    "        output = self.linear(lstmout)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.n_layers * 2, batch_size, self.hidden_dim).zero_(),\n",
    "                  weight.new(self.n_layers * 2, batch_size, self.hidden_dim).zero_())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "mynet = gender(input_dim = 54, output_dim = 2, hidden_dim = 50, n_layers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainx, trainy, testx, testy, batch_size = 500, epochs = 50, lr = 0.01, clip = 0.01, print_every = 10):\n",
    "    \n",
    "    net.train()\n",
    "    net.double()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr = lr)\n",
    "    \n",
    "    net\n",
    "    counter = 0\n",
    "    val_losses = []\n",
    "    train_permutation = torch.randperm(trainx.shape[0])\n",
    "    test_permutation = torch.randperm(testx.shape[0])\n",
    "\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        counter += 1\n",
    "        \n",
    "        #Initializing hidden state\n",
    "        hd = net.init_hidden(batch_size)\n",
    "        for i in range(0,trainx.shape[0], batch_size):\n",
    "            \n",
    "            indices = train_permutation[i:i+batch_size]\n",
    "\n",
    "        \n",
    "            inputs, targets = torch.from_numpy(trainx[indices]), torch.from_numpy(trainy[indices])\n",
    "            hd = tuple([each.data for each in hd])\n",
    "\n",
    "            net.zero_grad()\n",
    "\n",
    "            outputs, hd = net(inputs, hd)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if counter % print_every == 0:\n",
    "                net.eval()\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                \n",
    "                for j in range(0, testx.shape[0], batch_size):\n",
    "                    \n",
    "                    indic = test_permutation[j:j + batch_size]\n",
    "                    vinputs, vtargets = torch.from_numpy(testx[indic]), torch.from_numpy(testy[indic])\n",
    "\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                    output_h, val_h = net(vinputs, val_h)\n",
    "                    val_loss = criterion(output_h, vtargets)\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "\n",
    "            print(\"Epoch: {}/{} ...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                  \"Val Loss:{:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50 ... Step: 1... Loss: 3.3926... Val Loss:nan\n",
      "Epoch: 501/50 ... Step: 1... Loss: 3.2703... Val Loss:nan\n",
      "Epoch: 1001/50 ... Step: 1... Loss: 2.9751... Val Loss:nan\n",
      "Epoch: 1501/50 ... Step: 1... Loss: 2.7687... Val Loss:nan\n",
      "Epoch: 2001/50 ... Step: 1... Loss: 2.6628... Val Loss:nan\n",
      "Epoch: 2501/50 ... Step: 1... Loss: 2.6826... Val Loss:nan\n",
      "Epoch: 3001/50 ... Step: 1... Loss: 2.3607... Val Loss:nan\n",
      "Epoch: 3501/50 ... Step: 1... Loss: 2.0570... Val Loss:nan\n",
      "Epoch: 4001/50 ... Step: 1... Loss: 1.9140... Val Loss:nan\n",
      "Epoch: 4501/50 ... Step: 1... Loss: 2.4118... Val Loss:nan\n",
      "Epoch: 5001/50 ... Step: 1... Loss: 1.6517... Val Loss:nan\n",
      "Epoch: 5501/50 ... Step: 1... Loss: 1.6997... Val Loss:nan\n",
      "Epoch: 6001/50 ... Step: 1... Loss: 1.7232... Val Loss:nan\n",
      "Epoch: 6501/50 ... Step: 1... Loss: 1.4841... Val Loss:nan\n",
      "Epoch: 7001/50 ... Step: 1... Loss: 1.6448... Val Loss:nan\n",
      "Epoch: 7501/50 ... Step: 1... Loss: 1.8735... Val Loss:nan\n",
      "Epoch: 8001/50 ... Step: 1... Loss: 1.5239... Val Loss:nan\n",
      "Epoch: 8501/50 ... Step: 1... Loss: 3.3411... Val Loss:nan\n",
      "Epoch: 9001/50 ... Step: 1... Loss: 1.4511... Val Loss:nan\n",
      "Epoch: 9501/50 ... Step: 1... Loss: 1.5748... Val Loss:nan\n",
      "Epoch: 10001/50 ... Step: 1... Loss: 1.4460... Val Loss:nan\n",
      "Epoch: 10501/50 ... Step: 1... Loss: 1.3970... Val Loss:nan\n",
      "Epoch: 11001/50 ... Step: 1... Loss: 1.3457... Val Loss:nan\n",
      "Epoch: 11501/50 ... Step: 1... Loss: 1.3244... Val Loss:nan\n",
      "Epoch: 12001/50 ... Step: 1... Loss: 1.2900... Val Loss:nan\n",
      "Epoch: 12501/50 ... Step: 1... Loss: 1.2239... Val Loss:nan\n",
      "Epoch: 13001/50 ... Step: 1... Loss: 1.1802... Val Loss:nan\n",
      "Epoch: 13501/50 ... Step: 1... Loss: 1.1509... Val Loss:nan\n",
      "Epoch: 14001/50 ... Step: 1... Loss: 1.0808... Val Loss:nan\n",
      "Epoch: 14501/50 ... Step: 1... Loss: 1.0532... Val Loss:nan\n",
      "Epoch: 15001/50 ... Step: 1... Loss: 1.0356... Val Loss:nan\n",
      "Epoch: 15501/50 ... Step: 1... Loss: 0.9940... Val Loss:nan\n",
      "Epoch: 16001/50 ... Step: 1... Loss: 0.9923... Val Loss:nan\n",
      "Epoch: 16501/50 ... Step: 1... Loss: 0.9712... Val Loss:nan\n",
      "Epoch: 17001/50 ... Step: 1... Loss: 0.9770... Val Loss:nan\n",
      "Epoch: 17501/50 ... Step: 1... Loss: 0.9506... Val Loss:nan\n",
      "Epoch: 18001/50 ... Step: 1... Loss: 0.9178... Val Loss:nan\n",
      "Epoch: 18501/50 ... Step: 1... Loss: 0.9179... Val Loss:nan\n",
      "Epoch: 19001/50 ... Step: 1... Loss: 0.9156... Val Loss:nan\n",
      "Epoch: 19501/50 ... Step: 1... Loss: 0.9167... Val Loss:nan\n",
      "Epoch: 20001/50 ... Step: 1... Loss: 0.8970... Val Loss:nan\n",
      "Epoch: 20501/50 ... Step: 1... Loss: 0.9006... Val Loss:nan\n",
      "Epoch: 21001/50 ... Step: 1... Loss: 0.8834... Val Loss:nan\n",
      "Epoch: 21501/50 ... Step: 1... Loss: 0.8768... Val Loss:nan\n",
      "Epoch: 22001/50 ... Step: 1... Loss: 0.8513... Val Loss:nan\n",
      "Epoch: 22501/50 ... Step: 1... Loss: 0.8594... Val Loss:nan\n",
      "Epoch: 23001/50 ... Step: 1... Loss: 0.8882... Val Loss:nan\n",
      "Epoch: 23501/50 ... Step: 1... Loss: 0.8862... Val Loss:nan\n",
      "Epoch: 24001/50 ... Step: 1... Loss: 0.8390... Val Loss:nan\n",
      "Epoch: 24501/50 ... Step: 1... Loss: 0.8545... Val Loss:nan\n",
      "Epoch: 25001/50 ... Step: 1... Loss: 0.8354... Val Loss:nan\n",
      "Epoch: 25501/50 ... Step: 1... Loss: 0.8233... Val Loss:nan\n",
      "Epoch: 26001/50 ... Step: 1... Loss: 0.8225... Val Loss:nan\n",
      "Epoch: 26501/50 ... Step: 1... Loss: 0.8337... Val Loss:nan\n",
      "Epoch: 27001/50 ... Step: 1... Loss: 0.7919... Val Loss:nan\n",
      "Epoch: 27501/50 ... Step: 1... Loss: 0.7928... Val Loss:nan\n",
      "Epoch: 28001/50 ... Step: 1... Loss: 0.7841... Val Loss:nan\n",
      "Epoch: 28501/50 ... Step: 1... Loss: 0.7882... Val Loss:nan\n",
      "Epoch: 29001/50 ... Step: 1... Loss: 0.7622... Val Loss:nan\n",
      "Epoch: 29501/50 ... Step: 1... Loss: 0.7799... Val Loss:nan\n",
      "Epoch: 30001/50 ... Step: 1... Loss: 0.8085... Val Loss:nan\n",
      "Epoch: 30501/50 ... Step: 1... Loss: 0.7788... Val Loss:nan\n",
      "Epoch: 31001/50 ... Step: 1... Loss: 0.7567... Val Loss:nan\n",
      "Epoch: 31501/50 ... Step: 1... Loss: 0.7514... Val Loss:nan\n",
      "Epoch: 32001/50 ... Step: 1... Loss: 0.7421... Val Loss:nan\n",
      "Epoch: 32501/50 ... Step: 1... Loss: 0.7304... Val Loss:nan\n",
      "Epoch: 33001/50 ... Step: 1... Loss: 0.7320... Val Loss:nan\n",
      "Epoch: 33501/50 ... Step: 1... Loss: 0.7323... Val Loss:nan\n",
      "Epoch: 34001/50 ... Step: 1... Loss: 0.7384... Val Loss:nan\n",
      "Epoch: 34501/50 ... Step: 1... Loss: 0.7367... Val Loss:nan\n",
      "Epoch: 35001/50 ... Step: 1... Loss: 0.7409... Val Loss:nan\n",
      "Epoch: 35501/50 ... Step: 1... Loss: 0.7335... Val Loss:nan\n",
      "Epoch: 36001/50 ... Step: 1... Loss: 0.7576... Val Loss:nan\n",
      "Epoch: 36501/50 ... Step: 1... Loss: 0.7262... Val Loss:nan\n",
      "Epoch: 37001/50 ... Step: 1... Loss: 0.7139... Val Loss:nan\n",
      "Epoch: 37501/50 ... Step: 1... Loss: 0.7067... Val Loss:nan\n",
      "Epoch: 38001/50 ... Step: 1... Loss: 0.7087... Val Loss:nan\n",
      "Epoch: 38501/50 ... Step: 1... Loss: 0.7060... Val Loss:nan\n",
      "Epoch: 39001/50 ... Step: 1... Loss: 0.7115... Val Loss:nan\n",
      "Epoch: 39501/50 ... Step: 1... Loss: 0.6823... Val Loss:nan\n",
      "Epoch: 40001/50 ... Step: 1... Loss: 0.7068... Val Loss:nan\n",
      "Epoch: 40501/50 ... Step: 1... Loss: 0.6951... Val Loss:nan\n",
      "Epoch: 41001/50 ... Step: 1... Loss: 0.6903... Val Loss:nan\n",
      "Epoch: 41501/50 ... Step: 1... Loss: 0.6838... Val Loss:nan\n",
      "Epoch: 42001/50 ... Step: 1... Loss: 0.7087... Val Loss:nan\n",
      "Epoch: 42501/50 ... Step: 1... Loss: 0.7030... Val Loss:nan\n",
      "Epoch: 43001/50 ... Step: 1... Loss: 0.6818... Val Loss:nan\n",
      "Epoch: 43501/50 ... Step: 1... Loss: 0.6868... Val Loss:nan\n",
      "Epoch: 44001/50 ... Step: 1... Loss: 0.7027... Val Loss:nan\n",
      "Epoch: 44501/50 ... Step: 1... Loss: 0.7083... Val Loss:nan\n",
      "Epoch: 45001/50 ... Step: 1... Loss: 0.6701... Val Loss:nan\n",
      "Epoch: 45501/50 ... Step: 1... Loss: 0.6812... Val Loss:nan\n",
      "Epoch: 46001/50 ... Step: 1... Loss: 0.6942... Val Loss:nan\n",
      "Epoch: 46501/50 ... Step: 1... Loss: 0.6862... Val Loss:nan\n",
      "Epoch: 47001/50 ... Step: 1... Loss: 0.6909... Val Loss:nan\n",
      "Epoch: 47501/50 ... Step: 1... Loss: 0.6869... Val Loss:nan\n",
      "Epoch: 48001/50 ... Step: 1... Loss: 0.6834... Val Loss:nan\n",
      "Epoch: 48501/50 ... Step: 1... Loss: 0.6900... Val Loss:nan\n",
      "Epoch: 49001/50 ... Step: 1... Loss: 0.6765... Val Loss:nan\n",
      "Epoch: 49501/50 ... Step: 1... Loss: 0.7034... Val Loss:nan\n",
      "Epoch: 50001/50 ... Step: 1... Loss: 0.6835... Val Loss:nan\n",
      "Epoch: 50501/50 ... Step: 1... Loss: 0.6710... Val Loss:nan\n",
      "Epoch: 51001/50 ... Step: 1... Loss: 0.6889... Val Loss:nan\n",
      "Epoch: 51501/50 ... Step: 1... Loss: 0.6814... Val Loss:nan\n",
      "Epoch: 52001/50 ... Step: 1... Loss: 0.6993... Val Loss:nan\n",
      "Epoch: 52501/50 ... Step: 1... Loss: 0.6847... Val Loss:nan\n",
      "Epoch: 53001/50 ... Step: 1... Loss: 0.6829... Val Loss:nan\n",
      "Epoch: 53501/50 ... Step: 1... Loss: 0.6965... Val Loss:nan\n",
      "Epoch: 54001/50 ... Step: 1... Loss: 0.6851... Val Loss:nan\n",
      "Epoch: 54501/50 ... Step: 1... Loss: 0.6747... Val Loss:nan\n",
      "Epoch: 55001/50 ... Step: 1... Loss: 0.6877... Val Loss:nan\n",
      "Epoch: 55501/50 ... Step: 1... Loss: 0.6944... Val Loss:nan\n",
      "Epoch: 56001/50 ... Step: 1... Loss: 0.6794... Val Loss:nan\n",
      "Epoch: 56501/50 ... Step: 1... Loss: 0.6729... Val Loss:nan\n",
      "Epoch: 57001/50 ... Step: 1... Loss: 0.6869... Val Loss:nan\n",
      "Epoch: 57501/50 ... Step: 1... Loss: 0.6905... Val Loss:nan\n",
      "Epoch: 58001/50 ... Step: 1... Loss: 0.6699... Val Loss:nan\n",
      "Epoch: 58501/50 ... Step: 1... Loss: 0.6908... Val Loss:nan\n",
      "Epoch: 59001/50 ... Step: 1... Loss: 0.7049... Val Loss:nan\n",
      "Epoch: 59501/50 ... Step: 1... Loss: 0.6947... Val Loss:nan\n",
      "Epoch: 60001/50 ... Step: 1... Loss: 0.6760... Val Loss:nan\n",
      "Epoch: 60501/50 ... Step: 1... Loss: 0.6737... Val Loss:nan\n",
      "Epoch: 61001/50 ... Step: 1... Loss: 0.6897... Val Loss:nan\n",
      "Epoch: 61501/50 ... Step: 1... Loss: 0.6803... Val Loss:nan\n",
      "Epoch: 62001/50 ... Step: 1... Loss: 0.7018... Val Loss:nan\n",
      "Epoch: 62501/50 ... Step: 1... Loss: 0.6743... Val Loss:nan\n",
      "Epoch: 63001/50 ... Step: 1... Loss: 0.6687... Val Loss:nan\n",
      "Epoch: 63501/50 ... Step: 1... Loss: 0.6694... Val Loss:nan\n",
      "Epoch: 64001/50 ... Step: 1... Loss: 0.6745... Val Loss:nan\n",
      "Epoch: 64501/50 ... Step: 1... Loss: 0.7091... Val Loss:nan\n",
      "Epoch: 65001/50 ... Step: 1... Loss: 0.7046... Val Loss:nan\n",
      "Epoch: 65501/50 ... Step: 1... Loss: 0.6724... Val Loss:nan\n",
      "Epoch: 66001/50 ... Step: 1... Loss: 0.7031... Val Loss:nan\n",
      "Epoch: 66501/50 ... Step: 1... Loss: 0.6734... Val Loss:nan\n",
      "Epoch: 67001/50 ... Step: 1... Loss: 0.6794... Val Loss:nan\n",
      "Epoch: 67501/50 ... Step: 1... Loss: 0.6749... Val Loss:nan\n",
      "Epoch: 68001/50 ... Step: 1... Loss: 0.6810... Val Loss:nan\n",
      "Epoch: 68501/50 ... Step: 1... Loss: 0.6919... Val Loss:nan\n",
      "Epoch: 69001/50 ... Step: 1... Loss: 0.6849... Val Loss:nan\n",
      "Epoch: 69501/50 ... Step: 1... Loss: 0.6896... Val Loss:nan\n",
      "Epoch: 70001/50 ... Step: 1... Loss: 0.6904... Val Loss:nan\n",
      "Epoch: 70501/50 ... Step: 1... Loss: 0.6800... Val Loss:nan\n",
      "Epoch: 71001/50 ... Step: 1... Loss: 0.6806... Val Loss:nan\n",
      "Epoch: 71501/50 ... Step: 1... Loss: 0.6748... Val Loss:nan\n",
      "Epoch: 72001/50 ... Step: 1... Loss: 0.6885... Val Loss:nan\n",
      "Epoch: 72501/50 ... Step: 1... Loss: 0.6961... Val Loss:nan\n",
      "Epoch: 73001/50 ... Step: 1... Loss: 0.6924... Val Loss:nan\n",
      "Epoch: 73501/50 ... Step: 1... Loss: 0.6833... Val Loss:nan\n",
      "Epoch: 74001/50 ... Step: 1... Loss: 0.6870... Val Loss:nan\n",
      "Epoch: 74501/50 ... Step: 1... Loss: 0.6816... Val Loss:nan\n",
      "Epoch: 75001/50 ... Step: 1... Loss: 0.6999... Val Loss:nan\n",
      "Epoch: 75501/50 ... Step: 1... Loss: 0.6885... Val Loss:nan\n",
      "Epoch: 76001/50 ... Step: 1... Loss: 0.6699... Val Loss:nan\n",
      "Epoch: 76501/50 ... Step: 1... Loss: 0.6758... Val Loss:nan\n",
      "Epoch: 77001/50 ... Step: 1... Loss: 0.6650... Val Loss:nan\n",
      "Epoch: 77501/50 ... Step: 1... Loss: 0.6933... Val Loss:nan\n",
      "Epoch: 78001/50 ... Step: 1... Loss: 0.6681... Val Loss:nan\n",
      "Epoch: 78501/50 ... Step: 1... Loss: 0.6774... Val Loss:nan\n",
      "Epoch: 79001/50 ... Step: 1... Loss: 0.6870... Val Loss:nan\n",
      "Epoch: 79501/50 ... Step: 1... Loss: 0.6876... Val Loss:nan\n",
      "Epoch: 80001/50 ... Step: 1... Loss: 0.6904... Val Loss:nan\n",
      "Epoch: 80501/50 ... Step: 1... Loss: 0.7151... Val Loss:nan\n",
      "Epoch: 81001/50 ... Step: 1... Loss: 0.6963... Val Loss:nan\n",
      "Epoch: 81501/50 ... Step: 1... Loss: 0.6929... Val Loss:nan\n",
      "Epoch: 82001/50 ... Step: 1... Loss: 0.6840... Val Loss:nan\n",
      "Epoch: 82501/50 ... Step: 1... Loss: 0.6660... Val Loss:nan\n",
      "Epoch: 83001/50 ... Step: 1... Loss: 0.6823... Val Loss:nan\n",
      "Epoch: 83501/50 ... Step: 1... Loss: 0.6782... Val Loss:nan\n",
      "Epoch: 84001/50 ... Step: 1... Loss: 0.6851... Val Loss:nan\n",
      "Epoch: 84501/50 ... Step: 1... Loss: 0.6908... Val Loss:nan\n",
      "Epoch: 85001/50 ... Step: 1... Loss: 0.6955... Val Loss:nan\n",
      "Epoch: 85501/50 ... Step: 1... Loss: 0.6831... Val Loss:nan\n",
      "Epoch: 86001/50 ... Step: 1... Loss: 0.6728... Val Loss:nan\n",
      "Epoch: 86501/50 ... Step: 1... Loss: 0.6681... Val Loss:nan\n",
      "Epoch: 87001/50 ... Step: 1... Loss: 0.6780... Val Loss:nan\n",
      "Epoch: 87501/50 ... Step: 1... Loss: 0.6724... Val Loss:nan\n",
      "Epoch: 88001/50 ... Step: 1... Loss: 0.6926... Val Loss:nan\n",
      "Epoch: 88501/50 ... Step: 1... Loss: 0.6821... Val Loss:nan\n",
      "Epoch: 89001/50 ... Step: 1... Loss: 0.6822... Val Loss:nan\n",
      "Epoch: 89501/50 ... Step: 1... Loss: 0.6748... Val Loss:nan\n",
      "Epoch: 90001/50 ... Step: 1... Loss: 0.6787... Val Loss:nan\n",
      "Epoch: 90501/50 ... Step: 1... Loss: 0.6913... Val Loss:nan\n",
      "Epoch: 91001/50 ... Step: 1... Loss: 0.6822... Val Loss:nan\n",
      "Epoch: 91501/50 ... Step: 1... Loss: 0.6775... Val Loss:nan\n",
      "Epoch: 92001/50 ... Step: 1... Loss: 0.6793... Val Loss:nan\n",
      "Epoch: 92501/50 ... Step: 1... Loss: 0.6758... Val Loss:nan\n",
      "Epoch: 93001/50 ... Step: 1... Loss: 0.6766... Val Loss:nan\n",
      "Epoch: 93501/50 ... Step: 1... Loss: 0.6744... Val Loss:nan\n",
      "Epoch: 94001/50 ... Step: 1... Loss: 0.7110... Val Loss:nan\n",
      "Epoch: 94501/50 ... Step: 1... Loss: 0.7583... Val Loss:nan\n",
      "Epoch: 95001/50 ... Step: 1... Loss: 0.7143... Val Loss:nan\n",
      "Epoch: 95501/50 ... Step: 1... Loss: 0.6809... Val Loss:nan\n",
      "Epoch: 96001/50 ... Step: 1... Loss: 0.7187... Val Loss:nan\n",
      "Epoch: 96501/50 ... Step: 1... Loss: 0.6981... Val Loss:nan\n",
      "Epoch: 97001/50 ... Step: 1... Loss: 0.7133... Val Loss:nan\n",
      "Epoch: 97501/50 ... Step: 1... Loss: 0.7023... Val Loss:nan\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (4, 104, 50), got (4, 500, 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-9b0f5da39388>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmynet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_train_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_train_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_test_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_test_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-70-790245739f21>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, trainx, trainy, testx, testy, batch_size, epochs, lr, clip, print_every)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-a48363ec44a3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mlstmout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstmout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mflat_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         func = self._backend.RNN(\n\u001b[1;32m    180\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             check_hidden_size(hidden[0], expected_hidden_size,\n\u001b[0;32m--> 147\u001b[0;31m                               'Expected hidden[0] size {}, got {}')\n\u001b[0m\u001b[1;32m    148\u001b[0m             check_hidden_size(hidden[1], expected_hidden_size,\n\u001b[1;32m    149\u001b[0m                               'Expected hidden[1] size {}, got {}')\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[0;34m(hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcheck_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Expected hidden size {}, got {}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden[0] size (4, 104, 50), got (4, 500, 50)"
     ]
    }
   ],
   "source": [
    "hidden = train(mynet, trainx = net_train_x, trainy = net_train_y, testx = net_test_x, testy = net_test_y, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build model in keras ( a stacked LSTM model with many-to-one arch ) here 30 sequence and 2 output each for one category(m/f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #build the model: 2 stacked LSTM\n",
    "# print('Build model...')\n",
    "# input_bilstm=Input(shape = (maxlen,len_vocab))\n",
    "# bi_one = Bidirectional(LSTM(1024, return_sequences=True))(input_bilstm)\n",
    "# drop1 = Dropout(0.2)(bi_one)\n",
    "# bi_two = Bidirectional(LSTM(1024, return_sequences=False))(drop1)\n",
    "# drop2 = Dropout(0.2)(bi_two)\n",
    "# output = Dense(2, activation='softmax')(drop2)\n",
    "# model = Model(input_bilstm, output)\n",
    "\n",
    "\n",
    "# opt = optimizers.adam(lr = 0.01)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#build the model: 2 stacked LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(maxlen,len_vocab)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "opt = optimizers.SGD(lr=0.1)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=500\n",
    "model.fit(net_train_x, net_train_y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=50,\n",
    "          validation_data=(net_test_x, net_test_y)\n",
    "         )\n",
    "model.save('Martin_program.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc = model.evaluate(vtest_x, vtest_y)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30979005, 0.6902099 ]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name=[\"riya\"]\n",
    "X=[]\n",
    "trunc_name = [i[0:maxlen] for i in name]\n",
    "for i in trunc_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i.lower())]\n",
    "    for k in range(0,maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index[\"END\"]))\n",
    "    X.append(tmp)\n",
    "pred=model.predict(np.asarray(X))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30979005, 0.6902099 ]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets train more, clearly some very simple female names it doesnt get right like mentioned above (inspite it exists in training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-22f80a43d14b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test score:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_X' is not defined"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(test_X, test_Y)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30979005, 0.69021   ],\n",
       "       [0.30979005, 0.69021   ],\n",
       "       [0.30979005, 0.69021   ],\n",
       "       [0.30979005, 0.69021   ],\n",
       "       [0.30979005, 0.69021   ],\n",
       "       [0.30979005, 0.69021   ],\n",
       "       [0.30979005, 0.69021   ],\n",
       "       [0.30979005, 0.69021   ],\n",
       "       [0.30979005, 0.69021   ]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name=[\"sandhya\",\"jaspreet\",\"rajesh\",\"kaveri\",\"aditi deepak\",\"arihant\",\"sasikala\",\"aditi\",\"ragini rajaram\"]\n",
    "X=[]\n",
    "trunc_name = [i[0:maxlen] for i in name]\n",
    "for i in trunc_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i)]\n",
    "    for k in range(0,maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index[\"END\"]))\n",
    "    X.append(tmp)\n",
    "pred=model.predict(np.asarray(X))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4178515 , 0.5821485 ],\n",
       "       [0.37405205, 0.62594795],\n",
       "       [0.02188767, 0.9781123 ]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name=[\"abhi\",\"abhi deepak\",\"mr. abhi\"]\n",
    "X=[]\n",
    "trunc_name = [i[0:maxlen] for i in name]\n",
    "for i in trunc_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i)]\n",
    "    for k in range(0,maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index[\"END\"]))\n",
    "    X.append(tmp)\n",
    "pred=model.predict(np.asarray(X))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09026915, 0.90973085],\n",
       "       [0.9785337 , 0.02146639],\n",
       "       [0.03325909, 0.9667409 ]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name=[\"rajini\",\"rajinikanth\",\"mr. rajini\"]\n",
    "X=[]\n",
    "trunc_name = [i[0:maxlen] for i in name]\n",
    "for i in trunc_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i)]\n",
    "    for k in range(0,maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index[\"END\"]))\n",
    "    X.append(tmp)\n",
    "pred=model.predict(np.asarray(X))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save our model and data\n",
    "model.save_weights('gender_model',overwrite=True)\n",
    "train.to_csv(\"train_split.csv\")\n",
    "test.to_csv(\"test_split.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = model.predict(test_X)\n",
    "prob_m = [i[0] for i in evals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.DataFrame(prob_m)\n",
    "out['name'] = test.name.reset_index()['name']\n",
    "out['m_or_f']=test.m_or_f.reset_index()['m_or_f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.head(10)\n",
    "out.columns = ['prob_m','name','actual']\n",
    "out.head(10)\n",
    "out.to_csv(\"gender_pred_out.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
