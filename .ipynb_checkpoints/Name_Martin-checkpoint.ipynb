{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Embedding, Bidirectional, Input, TimeDistributed\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras import optimizers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import optim,nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameters\n",
    "maxlen = 30\n",
    "labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "czech = pd.read_excel('czech.xlsx', encoding='latin',header = None)\n",
    "czech.columns = ['Name', 'm_or_f']\n",
    "czech['namelen'] = [len(str(i)) for i in czech['Name']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing of new ninety file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>m_or_f</th>\n",
       "      <th>namelen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abigail</td>\n",
       "      <td>f</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ada</td>\n",
       "      <td>f</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adalberta</td>\n",
       "      <td>f</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adéla</td>\n",
       "      <td>f</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelaida</td>\n",
       "      <td>f</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Name m_or_f  namelen\n",
       "0    Abigail      f        7\n",
       "1        Ada      f        3\n",
       "2  Adalberta      f        9\n",
       "3      Adéla      f        5\n",
       "4   Adelaida      f        8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cname = czech['Name']\n",
    "collect = []\n",
    "\n",
    "\n",
    "for i in range(len(cname)):\n",
    "    collect.extend(list(str(cname[i]).lower()))\n",
    "# collect.extend(['END'])\n",
    "collect = set(collect)\n",
    "\n",
    "czech.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.read_csv(\"gender_data.csv\",header=None)\n",
    "data_set.columns = ['name','m_or_f']\n",
    "data_set['namelen']= [len(str(i)) for i in data_set['name']]\n",
    "data_set1 = data_set[(data_set['namelen'] >= 2) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = data_set['name']\n",
    "gender = data_set['m_or_f']\n",
    "vocab = set(' '.join([str(i) for i in names]))\n",
    "vocab.add('END')\n",
    "vocab = vocab.union(collect)\n",
    "len_vocab = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t', 'ň', 'ž', 'h', 'a', 'x', 'i', 'k', '7', 's', '2', 'é', 'ú', 'ů', 'z', 'l', '.', 'j', 'ý', 'č', 'e', 'n', 'p', 'š', '5', 'r', 'c', '0', 'ó', '6', 'ť', '1', 'í', 'q', ' ', '4', 'y', '3', '8', 'á', 'ř', 'v', 'ď', 'g', 'u', 'f', 'ě', 'b', 'd', 'o', 'END', '9', 'm', 'w'}\n",
      "vocab length is  54\n",
      "length of data_set is  15226\n"
     ]
    }
   ],
   "source": [
    "print(vocab)\n",
    "print(\"vocab length is \",len_vocab)\n",
    "print (\"length of data_set is \",len(data_set1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_index = dict((c, i) for i, c in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ú': 12, 'ň': 1, 'ž': 2, 'h': 3, 'y': 36, 'a': 4, 'x': 5, 'k': 7, 't': 0, 's': 9, '2': 10, 'é': 11, 'ů': 13, 'ý': 18, 'l': 15, 'j': 17, 'z': 14, 'n': 21, 'š': 23, 'p': 22, '.': 16, 'r': 25, 'c': 26, '0': 27, 'ó': 28, '5': 24, 'ť': 30, '1': 31, 'č': 19, 'í': 32, ' ': 34, '4': 35, 'á': 39, 'i': 6, 'e': 20, '3': 37, '8': 38, '7': 8, 'ř': 40, 'ď': 42, 'g': 43, 'u': 44, 'd': 48, 'f': 45, 'ě': 46, 'b': 47, 'q': 33, 'o': 49, '6': 29, 'w': 53, 'END': 50, '9': 51, 'm': 52, 'v': 41}\n"
     ]
    }
   ],
   "source": [
    "print(char_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "msk = np.random.rand(len(data_set1)) < 0.9\n",
    "train = data_set1[msk]\n",
    "test = data_set1[~msk]     \n",
    "\n",
    "msk = np.random.rand(len(czech)) < 0.9\n",
    "\n",
    "\n",
    "vtrain = czech[msk]\n",
    "vtest = czech[~msk]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_flag(i):\n",
    "    tmp = np.zeros(len_vocab);\n",
    "    tmp[i] = 1\n",
    "    return(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_flag(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### modify the code above to also convert each index to one-hot encoded representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# These are set of indian names, just for adding a little robustness and to construct that latent structure in names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take data_set upto max and truncate rest\n",
    "#encode to vector space(one hot encoding)\n",
    "#padd 'END' to shorter sequences\n",
    "#also convert each index to one-hot encoding\n",
    "train_x = []\n",
    "train_y = []\n",
    "trunc_train_name = [str(i)[0:maxlen] for i in train.name]\n",
    "for i in trunc_train_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i)]\n",
    "    for k in range(0,maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index[\"END\"]))\n",
    "    train_x.append(tmp)\n",
    "for i in train.m_or_f:\n",
    "    if i == 'm':\n",
    "        train_y.append(1)\n",
    "    else:\n",
    "        train_y.append(0)\n",
    "    \n",
    "train_x=np.asarray(train_x)\n",
    "train_y=np.asarray(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = []\n",
    "test_y = []\n",
    "trunc_test_name = [str(i)[0:maxlen] for i in test.name]\n",
    "for i in trunc_test_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i)]\n",
    "    for k in range(0,maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index[\"END\"]))\n",
    "    test_x.append(tmp)\n",
    "for i in test.m_or_f:\n",
    "    if i == 'm':\n",
    "        test_y.append(1)\n",
    "    else:\n",
    "        test_y.append(0)\n",
    "    \n",
    "test_x = np.asarray(test_x)\n",
    "test_y = np.asarray(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# These are Czech names, well, that's what the program is all about "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtrain_x = []\n",
    "vtrain_y = []\n",
    "\n",
    "train_name = [str(i) for i in vtrain.Name]\n",
    "for i in train_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i.lower())]\n",
    "    for k in range(0, maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index['END']))\n",
    "    vtrain_x.append(tmp)\n",
    "for i in vtrain.m_or_f:\n",
    "    if i == 'm':\n",
    "        vtrain_y.append(1)\n",
    "    else:\n",
    "        vtrain_y.append(0)\n",
    "vtrain_x = np.asarray(vtrain_x)\n",
    "vtrain_y = np.asarray(vtrain_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtest_x = []\n",
    "vtest_y = []\n",
    "\n",
    "train_name = [str(i) for i in vtest.Name]\n",
    "for i in train_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i.lower())]\n",
    "    for k in range(0, maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index['END']))\n",
    "    vtest_x.append(tmp)\n",
    "for i in vtest.m_or_f:\n",
    "    if i == 'm':\n",
    "        vtest_y.append(1)\n",
    "    else:\n",
    "        vtest_y.append(0)\n",
    "                       \n",
    "vtest_x = np.asarray(vtest_x)\n",
    "vtest_y = np.asarray(vtest_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining all training set, and all test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are all training data!\n",
      "Shape of indian training name: (13738, 30, 54)\n",
      "Shape of Czech training name: (1125, 30, 54)\n"
     ]
    }
   ],
   "source": [
    "print(\"These are all training data!\")\n",
    "print(\"Shape of indian training name:\", train_x.shape)\n",
    "print(\"Shape of Czech training name:\", vtrain_x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining them all...\n",
      "Now net shape is: (14863, 30, 54)\n",
      "Now they're combined, but we must shuffle them too...shuffling\n",
      "Shuffled!!\n"
     ]
    }
   ],
   "source": [
    "print(\"Combining them all...\")\n",
    "net_train_x = np.concatenate([train_x, vtrain_x], axis = 0)\n",
    "net_train_y = np.concatenate([train_y, vtrain_y], axis = 0)\n",
    "print(\"Now net shape is:\", net_train_x.shape)\n",
    "print(\"Now they're combined, but we must shuffle them too...shuffling\")\n",
    "np.random.shuffle(net_train_x)\n",
    "np.random.shuffle(net_train_y)\n",
    "print(\"Shuffled!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are all test data!\n",
      "Shape of indian test name: (1488, 30, 54)\n",
      "Shape of Czech test name: (125, 30, 54)\n"
     ]
    }
   ],
   "source": [
    "print(\"These are all test data!\")\n",
    "print(\"Shape of indian test name:\", test_x.shape)\n",
    "print(\"Shape of Czech test name:\", vtest_x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining them all...\n",
      "Now net shape is: (1613, 30, 54)\n",
      "Now they're combined, but we must shuffle them too...shuffling\n",
      "Shuffled!!\n"
     ]
    }
   ],
   "source": [
    "print(\"Combining them all...\")\n",
    "net_test_x = np.concatenate([test_x, vtest_x], axis = 0)\n",
    "net_test_y = np.concatenate([test_y, vtest_y], axis = 0)\n",
    "print(\"Now net shape is:\", net_test_x.shape)\n",
    "print(\"Now they're combined, but we must shuffle them too...shuffling\")\n",
    "np.random.shuffle(net_test_x)\n",
    "np.random.shuffle(net_test_y)\n",
    "print(\"Shuffled!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making BatchLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size of one instance from iter is:  torch.Size([100, 30, 54])\n",
      "Sample target size of that same one instance from iter is torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "#Creating Tensor Datasets\n",
    "train_data = TensorDataset(torch.from_numpy(net_train_x).double(), torch.from_numpy(net_train_y).long())\n",
    "test_data = TensorDataset(torch.from_numpy(net_test_x).double(), torch.from_numpy(net_test_y).long())\n",
    "\n",
    "#Now creating Data Loaders\n",
    "batch = 100\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle = True, batch_size = batch)\n",
    "test_loader = DataLoader(test_data, shuffle = True, batch_size = batch)\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print(\"Sample input size of one instance from iter is: \", sample_x.size()) #Batch size, Sequence Length, Feature Dimension\n",
    "print(\"Sample target size of that same one instance from iter is\", sample_y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model in PyTorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.DoubleTensor\n",
    "\n",
    "class gender(nn.Module):\n",
    "    def __init__(self, embedding_dim, no_units, output_dim, n_layers):\n",
    "        super(gender, self).__init__()\n",
    "        self.no_units = no_units\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(embedding_dim, no_units, n_layers, batch_first = True, bidirectional = True)\n",
    "        self.linear = nn.Linear(no_units * 2, output_dim)\n",
    "#         self.soft = nn.Softmax(dim = 1)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        hidden = self.init_hidden(x.size(0))\n",
    "        lstmout, hidden = self.lstm(x.float(), hidden)\n",
    "#         lstmout = lstmout.contiguous().view(-1, self.no_units)\n",
    "\n",
    "        output = self.linear(lstmout)\n",
    "        \n",
    "#         output = self.soft(output)\n",
    "        return output[:, -1, :], hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers * 2, batch_size, self.no_units).zero_().cuda(),\n",
    "                      weight.new(self.n_layers * 2, batch_size, self.no_units).zero_().cuda())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mynet = gender(embedding_dim = 54, output_dim = 2, no_units = 50, n_layers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainn(net, batch_size = 100, epochs = 50, lr = 0.01, clip = 5, print_every = 10):\n",
    "    global check\n",
    "    net.train()\n",
    "    net.float()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr = lr)\n",
    "    \n",
    "    net().cuda()\n",
    "    counter = 0\n",
    "    val_losses = []\n",
    "\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        #Initializing hidden state\n",
    "        hd = net.init_hidden(batch_size)\n",
    "        for batch_i, target_i in train_loader:\n",
    "            \n",
    "\n",
    "        \n",
    "            inputs, targets = batch_i.cuda(), target_i.cuda()\n",
    "\n",
    "            hd = tuple([each.data for each in hd])\n",
    "\n",
    "            net.zero_grad()\n",
    "\n",
    "            outputs, hd = net(inputs, hd)\n",
    "            print(type(outputs))\n",
    "            print(targets)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 1 == 0:\n",
    "                net.eval()\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                \n",
    "                for batch_j, target_j in test_loader:\n",
    "                    \n",
    "                    vinputs, vtargets = batch_j.cuda(), target_j.cuda()\n",
    "\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                    output_h, val_h = net(vinputs, val_h)\n",
    "                    val_loss = criterion(output_h, vtargets)\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "\n",
    "            print(\"Epoch: {}/{} ...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                  \"Val Loss:{:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainn(mynet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_train_x[indices].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build model in keras ( a stacked LSTM model with many-to-one arch ) here 30 sequence and 2 output each for one category(m/f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc = model.evaluate(vtest_x, vtest_y)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name=[\"riya\"]\n",
    "X=[]\n",
    "trunc_name = [i[0:maxlen] for i in name]\n",
    "for i in trunc_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i.lower())]\n",
    "    for k in range(0,maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index[\"END\"]))\n",
    "    X.append(tmp)\n",
    "pred=model.predict(np.asarray(X))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets train more, clearly some very simple female names it doesnt get right like mentioned above (inspite it exists in training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc = model.evaluate(test_X, test_Y)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name=[\"sandhya\",\"jaspreet\",\"rajesh\",\"kaveri\",\"aditi deepak\",\"arihant\",\"sasikala\",\"aditi\",\"ragini rajaram\"]\n",
    "X=[]\n",
    "trunc_name = [i[0:maxlen] for i in name]\n",
    "for i in trunc_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i)]\n",
    "    for k in range(0,maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index[\"END\"]))\n",
    "    X.append(tmp)\n",
    "pred=model.predict(np.asarray(X))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name=[\"abhi\",\"abhi deepak\",\"mr. abhi\"]\n",
    "X=[]\n",
    "trunc_name = [i[0:maxlen] for i in name]\n",
    "for i in trunc_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i)]\n",
    "    for k in range(0,maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index[\"END\"]))\n",
    "    X.append(tmp)\n",
    "pred=model.predict(np.asarray(X))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name=[\"rajini\",\"rajinikanth\",\"mr. rajini\"]\n",
    "X=[]\n",
    "trunc_name = [i[0:maxlen] for i in name]\n",
    "for i in trunc_name:\n",
    "    tmp = [set_flag(char_index[j]) for j in str(i)]\n",
    "    for k in range(0,maxlen - len(str(i))):\n",
    "        tmp.append(set_flag(char_index[\"END\"]))\n",
    "    X.append(tmp)\n",
    "pred=model.predict(np.asarray(X))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save our model and data\n",
    "model.save_weights('gender_model',overwrite=True)\n",
    "train.to_csv(\"train_split.csv\")\n",
    "test.to_csv(\"test_split.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = model.predict(test_X)\n",
    "prob_m = [i[0] for i in evals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.DataFrame(prob_m)\n",
    "out['name'] = test.name.reset_index()['name']\n",
    "out['m_or_f']=test.m_or_f.reset_index()['m_or_f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.head(10)\n",
    "out.columns = ['prob_m','name','actual']\n",
    "out.head(10)\n",
    "out.to_csv(\"gender_pred_out.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
